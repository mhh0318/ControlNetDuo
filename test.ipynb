{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n",
      "Use Checkpoint: False\n",
      "Checkpoint Number: [0, 0, 0, 0]\n",
      "Use global window for all blocks in stage3\n",
      "load checkpoint from local path: /home/hu/MMIB/controlnet/annotator/ckpts/upernet_global_small.pth\n",
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM: Running in eps-prediction mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hu/miniconda3/envs/th/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v1.10.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loaded model config from [./models/mcldm_v15.yaml]\n"
     ]
    }
   ],
   "source": [
    "from share import *\n",
    "import config\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import random\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from annotator.util import resize_image, HWC3\n",
    "from annotator.uniformer import UniformerDetector\n",
    "from annotator.canny import CannyDetector\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "\n",
    "\n",
    "apply_canny = CannyDetector()\n",
    "apply_uniformer = UniformerDetector()\n",
    "\n",
    "model = create_model('./models/mcldm_v15.yaml').cpu()\n",
    "# model.load_state_dict(load_state_dict('./models/control_sd15_seg.pth', location='cuda'))\n",
    "# model = model.cuda()\n",
    "# ddim_sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd1 = torch.load('./models/control_sd15_seg.pth', map_location='cpu')\n",
    "sd2 = torch.load('./models/control_sd15_canny.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sd = {}\n",
    "for k,v in sd1.items():\n",
    "    if 'control_model' in k:\n",
    "        new_sd['control_model_1'+k.split('control_model')[1]] = v\n",
    "    else:\n",
    "        new_sd[k] = v\n",
    "\n",
    "for k,v in sd2.items():\n",
    "    if 'control_model' in k:\n",
    "        new_sd['control_model_2'+k.split('control_model')[1]] = v\n",
    "\n",
    "\n",
    "model.load_state_dict(new_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "input_image = np.array(Image.open('/home/hu/MMIB/remove/sflckr_images/alaska_lakes/43259216952_59352d7204_b.jpg').resize((512,512)))\n",
    "input_image_1  = np.array(Image.open('/home/hu/MMIB/controlnet/test_imgs/pose2.png').resize((512,512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hu/MMIB/controlnet/annotator/uniformer/mmseg/models/segmentors/base.py:271: UserWarning: show==False and out_file is not specified, only result image will be returned\n",
      "  warnings.warn('show==False and out_file is not specified, only '\n"
     ]
    }
   ],
   "source": [
    "input_image = HWC3(input_image)\n",
    "detected_map = apply_uniformer(resize_image(input_image, 512))\n",
    "img = resize_image(input_image, 512)\n",
    "H, W, C = img.shape\n",
    "\n",
    "detected_map_seg = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "control_1 = torch.from_numpy(detected_map_seg.copy()).float().cuda() / 255.0\n",
    "control_1 = torch.stack([control_1 for _ in range(1)], dim=0)\n",
    "control_1 = einops.rearrange(control_1, 'b h w c -> b c h w').clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = resize_image(HWC3(input_image_1), 512)\n",
    "H, W, C = img.shape\n",
    "\n",
    "detected_map_cn = apply_canny(img, low_threshold=100, high_threshold=200)\n",
    "detected_map_cn = HWC3(detected_map_cn)\n",
    "\n",
    "control_2 = torch.from_numpy(detected_map_cn.copy()).float().cuda() / 255.0\n",
    "control_2 = torch.stack([control_2 for _ in range(1)], dim=0)\n",
    "control_2 = einops.rearrange(control_2, 'b h w c -> b c h w').clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "n_samples = 1\n",
    "\n",
    "prompt = ''\n",
    "a_prompt = 'best quality, extremely detailed'\n",
    "\n",
    "n_prompt = 'longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality'\n",
    "\n",
    "cond = {\"c_concat\": [control_1,control_2], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * n_samples)]}\n",
    "un_cond = {\"c_concat\": [control_1,control_2], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * n_samples)]}\n",
    "shape = (4, H // 8, W // 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
       "           [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
       "           [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
       "           ...,\n",
       "           [0.2392, 0.2392, 0.2392,  ..., 0.4706, 0.4706, 0.4706],\n",
       "           [0.2392, 0.2392, 0.2392,  ..., 0.4706, 0.4706, 0.4706],\n",
       "           [0.2392, 0.2392, 0.2392,  ..., 0.4706, 0.4706, 0.4706]],\n",
       " \n",
       "          [[0.9020, 0.9020, 0.9020,  ..., 0.9020, 0.9020, 0.9020],\n",
       "           [0.9020, 0.9020, 0.9020,  ..., 0.9020, 0.9020, 0.9020],\n",
       "           [0.9020, 0.9020, 0.9020,  ..., 0.9020, 0.9020, 0.9020],\n",
       "           ...,\n",
       "           [0.9020, 0.9020, 0.9020,  ..., 0.4706, 0.4706, 0.4706],\n",
       "           [0.9020, 0.9020, 0.9020,  ..., 0.4706, 0.4706, 0.4706],\n",
       "           [0.9020, 0.9020, 0.9020,  ..., 0.4706, 0.4706, 0.4706]],\n",
       " \n",
       "          [[0.9020, 0.9020, 0.9020,  ..., 0.9020, 0.9020, 0.9020],\n",
       "           [0.9020, 0.9020, 0.9020,  ..., 0.9020, 0.9020, 0.9020],\n",
       "           [0.9020, 0.9020, 0.9020,  ..., 0.9020, 0.9020, 0.9020],\n",
       "           ...,\n",
       "           [0.9804, 0.9804, 0.9804,  ..., 0.2745, 0.2745, 0.2745],\n",
       "           [0.9804, 0.9804, 0.9804,  ..., 0.2745, 0.2745, 0.2745],\n",
       "           [0.9804, 0.9804, 0.9804,  ..., 0.2745, 0.2745, 0.2745]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond['c_concat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 5010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:16<00:00,  3.02it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed_everything(5010)\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "samples, intermediates = ddim_sampler.sample(50, 1,\n",
    "                                                shape, cond, verbose=False, eta=0.0,\n",
    "                                                unconditional_guidance_scale=9.,\n",
    "                                                unconditional_conditioning=un_cond,\n",
    "                                                sc_0=0.7, sc_1=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = model.decode_first_stage(samples)\n",
    "img = torch.clamp(img/2+0.5, 0, 1)\n",
    "from torchvision.utils import save_image\n",
    "save_image(control_1, 'cn.png')\n",
    "save_image(control_2, 'seg.png')\n",
    "save_image(img, 'mm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "th",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3668f6eb5e0f26ac3b1ad746ce254d0365f1419494c8be761405e62fc3eecff0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
